# An√°lise Te√≥rica de Transformers de Uma Camada Baseados em Aten√ß√£o

<imagem: Diagrama detalhado mostrando a arquitetura de um transformer de uma camada, destacando os componentes de aten√ß√£o, as conex√µes skip-trigram e o fluxo de informa√ß√£o desde a entrada at√© a sa√≠da. O diagrama deve incluir visualiza√ß√µes das matrizes de peso e dos padr√µes de aten√ß√£o, bem como exemplos de skip-trigrams sendo formados>

### Introdu√ß√£o

Os **transformers de uma camada baseados exclusivamente em aten√ß√£o** representam uma classe fundamental de modelos que nos permitem compreender os princ√≠pios b√°sicos do funcionamento dos Large Language Models (LLMs). ==Este cap√≠tulo apresenta uma an√°lise te√≥rica aprofundada de como esses modelos podem ser entendidos como um **ensemble de um modelo de bigramas e v√°rios modelos de "skip-trigramas"** [^1].==

A import√¢ncia deste estudo reside na capacidade de decompor um transformer aparentemente complexo em componentes interpret√°veis, permitindo uma compreens√£o mais profunda dos mecanismos fundamentais de processamento de linguagem. Esta decomposi√ß√£o n√£o apenas fornece insights te√≥ricos valiosos, mas tamb√©m estabelece uma base para entender modelos mais complexos [^2].

### 1. Estrutura Matem√°tica Fundamental

==A base matem√°tica para compreender transformers de uma camada come√ßa com o **path expansion trick**, uma t√©cnica que nos permite decompor o modelo em termos interpret√°veis [^3].==

Um transformer de uma camada pode ser representado matematicamente como:

$$T = Id \otimes W_U \cdot \left(Id + \sum_{h \in H} A^h \otimes W_{OV}^h\right) \cdot Id \otimes W_E$$

onde:
- $W_U$ representa a matriz de unembedding
- $W_E$ representa a matriz de embedding
- $A^h$ s√£o os padr√µes de aten√ß√£o para cada cabe√ßa h
- $W_{OV}^h$ s√£o as matrizes de output-value para cada cabe√ßa

**Lemma 1:** *Para um transformer de uma camada, o produto dos termos pode ser expandido em uma soma onde cada termo corresponde a um caminho end-to-end atrav√©s do modelo.*

**Prova:**
Aplicando a propriedade distributiva e as regras do produto tensorial:

$$T = Id \otimes W_U W_E + \sum_{h \in H} A^h \otimes (W_U W_{OV}^h W_E)$$

==onde o primeiro termo representa o "caminho direto" e os termos subsequentes representam os efeitos das cabe√ßas de aten√ß√£o. $\blacksquare$==

> üìê **Coment√°rio Avan√ßado:** A representa√ß√£o matem√°tica do transformer como $T = Id \otimes W_U \cdot \left(Id + \sum_{h \in H} A^h \otimes W_{OV}^h\right) \cdot Id \otimes W_E$ destaca a ess√™ncia linear das opera√ß√µes internas do modelo. **Geometricamente, o "path expansion trick" permite que visualizemos o fluxo de informa√ß√£o como caminhos distintos atrav√©s de espa√ßos vetoriais de alta dimens√£o, onde cada caminho corresponde a uma transforma√ß√£o linear espec√≠fica aplicada aos embeddings dos tokens.** Essa perspectiva facilita a decomposi√ß√£o do modelo em componentes interpret√°veis, onde as cabe√ßas de aten√ß√£o atuam como proje√ß√µes que modificam a dire√ß√£o e magnitude dos vetores no espa√ßo, contribuindo individualmente para a sa√≠da final.

### 2. An√°lise dos Circuitos QK e OV

Um aspecto crucial para entender o funcionamento dos transformers de uma camada √© a separa√ß√£o dos circuitos Query-Key (QK) e Output-Value (OV) [^4]. 

Para cada cabe√ßa de aten√ß√£o h, temos dois componentes principais:

1. **Circuito QK:** $W_E^T W_Q^T W_K W_E$ - Determina os scores de aten√ß√£o entre tokens
2. **Circuito OV:** $W_U W_O^T W_V W_E$ - Define como um token afeta os logits se for atendido

```mermaid
graph TD
    A[Token de Entrada] --> B[Embedding]
    B --> C[Circuito QK]
    B --> D[Circuito OV]
    C --> E[Padr√µes de Aten√ß√£o]
    D --> F[Transforma√ß√£o de Valores]
    E --> G[Pesos de Aten√ß√£o]
    F --> H[Logits de Sa√≠da]
    G --> H
```

**Corol√°rio 1:** ==*O padr√£o de aten√ß√£o √© uma fun√ß√£o tanto do token fonte quanto do token destino, mas uma vez que um token destino decidiu quanto atender a um token fonte, o efeito na sa√≠da √© unicamente fun√ß√£o do token fonte.*==

> üìê **Coment√°rio Avan√ßado:** A separa√ß√£o dos circuitos Query-Key (QK) e Output-Value (OV) reflete a dualidade fundamental nas opera√ß√µes de aten√ß√£o em **√Ålgebra Linear**. ==**Geometricamente, o circuito QK calcula a similaridade entre tokens no espa√ßo vetorial, avaliando qu√£o alinhados est√£o os vetores de query e key, enquanto o circuito OV transforma os vetores de valor dos tokens atendidos, influenciando a sa√≠da do modelo.**== Essa distin√ß√£o permite compreender como o modelo determina quais tokens s√£o relevantes (atrav√©s de proje√ß√µes e medidas de similaridade) e como essas informa√ß√µes s√£o incorporadas na predi√ß√£o final, enfatizando a import√¢ncia das transforma√ß√µes lineares na captura de rela√ß√µes contextuais.

### 3. Skip-Trigramas e Sua Interpreta√ß√£o

Os **skip-trigramas** s√£o sequ√™ncias da forma [fonte]... [destino][sa√≠da], onde o modelo aprende a modificar as probabilidades da [sa√≠da] baseado nas rela√ß√µes entre [fonte] e [destino] [^5].

> ‚ö†Ô∏è **Ponto Crucial**: As matrizes expandidas QK e OV podem ter bilh√µes de entradas, transformando efetivamente o modelo em uma "sala chinesa" comprimida.

Considere o seguinte exemplo de skip-trigrama:

$$\text{[perfect]... [are] ‚Üí [perfect]}$$
$$\text{[perfect]... [looks] ‚Üí [super]}$$

Este padr√£o demonstra como o modelo pode aprender tanto a copiar tokens quanto a predizer continua√ß√µes semanticamente apropriadas [^6].

> üìê **Coment√°rio Avan√ßado:** Os **skip-trigramas** ampliam a capacidade dos transformers de capturar depend√™ncias n√£o adjacentes, essenciais para modelar a linguagem natural. ==**Geometricamente, isso representa a capacidade do modelo de conectar pontos distantes no espa√ßo vetorial dos embeddings, criando "atalhos" que permitem a transfer√™ncia de informa√ß√£o entre tokens separados por v√°rias posi√ß√µes.**== A expans√£o das matrizes QK e OV em dimens√µes elevadas reflete a riqueza de padr√µes que o modelo pode aprender, onde cada entrada na matriz corresponde a uma poss√≠vel intera√ß√£o entre tokens. Essa estrutura matricial massiva √© manipulada eficientemente gra√ßas √†s propriedades das transforma√ß√µes lineares e √† esparsidade das aten√ß√µes aprendidas.

[^1]: "We claim that one-layer attention-only transformers can be understood as an ensemble of a bigram model and several 'skip-trigram' models (affecting the probabilities of sequences 'A... BC')." *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^2]: "The goal of this section is to rigorously show this correspondence, and demonstrate how to convert the raw weights of a transformer into interpretable tables of skip-trigram probability adjustments." *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^3]: "Our key trick is to simply expand the product. This transforms the product (where every term corresponds to a layer), into a sum where every term corresponds to an end-to-end path." *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^4]: "For each attention head h we have a term A^h ‚äó (W_U W_{OV}^h W_E) where A^h = softmax (t^T ¬∑ W_E^T W_{QK}^h W_E ¬∑ t). How can we map these terms to model behavior?" *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^5]: "By multiplying out the OV and QK circuits, we've succeeded in doing this: the neural network parameters are now simple linear or bilinear functions on tokens." *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^6]: "One of the most striking things about looking at these matrices is that most attention heads in one layer models dedicate an enormous fraction of their capacity to copying." *(Trecho de A Mathematical Framework for Transformer Circuits)*

### 4. Mecanismo de Aten√ß√£o Seletiva e Ajuste de Probabilidades

==O poder dos transformers de uma camada reside em sua capacidade de **aten√ß√£o seletiva**, onde cada cabe√ßa de aten√ß√£o pode atender seletivamente do token presente ("B") a um token anterior ("A") e copiar informa√ß√µes para ajustar a probabilidade de poss√≠veis tokens seguintes ("C") [^7].==

Este mecanismo pode ser formalizado matematicamente da seguinte forma:

$$p(C|B,A) = \text{softmax}(W_U(W_O W_V(A_{att}x_A) + x_B))$$

onde:
- $x_A$ e $x_B$ s√£o os embeddings dos tokens A e B
- $A_{att}$ √© o padr√£o de aten√ß√£o que determina quanto B atende a A
- $W_O W_V$ √© a matriz que controla como a informa√ß√£o √© copiada

**Lemma 2:** *A capacidade de ajuste de probabilidade de um transformer de uma camada √© limitada pela dimensionalidade do espa√ßo de embedding e pelo n√∫mero de cabe√ßas de aten√ß√£o.*

**Prova:**
Seja $d$ a dimensionalidade do embedding e $h$ o n√∫mero de cabe√ßas de aten√ß√£o. O espa√ßo de transforma√ß√µes poss√≠veis √© limitado por:

1. Dimens√£o do espa√ßo de embedding: $\mathbb{R}^d$
2. Rank m√°ximo das matrizes $W_O W_V$: $\min(d, d_{head})$
3. N√∫mero total de transforma√ß√µes independentes: $h$

Portanto, o espa√ßo total de ajustes poss√≠veis √© limitado por $h \cdot \min(d, d_{head})$. $\blacksquare$

> üìê **Coment√°rio Avan√ßado:** A capacidade de **aten√ß√£o seletiva** dos transformers √© um testemunho do poder das proje√ß√µes lineares em espa√ßos de alta dimens√£o. ==**Geometricamente, cada cabe√ßa de aten√ß√£o atua como um filtro direcional, focando em componentes espec√≠ficas dos vetores de embedding que s√£o relevantes para a tarefa corrente.**== O Lema 2 ressalta que a capacidade do modelo de ajustar probabilidades est√° intrinsecamente ligada ao rank das matrizes de proje√ß√£o e ao espa√ßo dispon√≠vel para representar varia√ß√µes nos dados. Isso reflete a import√¢ncia da dimensionalidade e do n√∫mero de cabe√ßas de aten√ß√£o em fornecer ao modelo a flexibilidade necess√°ria para capturar padr√µes complexos na linguagem.

### 5. Arquitetura Detalhada do Mecanismo de C√≥pia

```mermaid
graph TD
    subgraph "Mecanismo de C√≥pia"
        B[Token Presente 'B'] --> Q[Query]
        A[Token Anterior 'A'] --> K[Key]
        A --> V[Value]
        Q --> S[Score]
        K --> S
        S --> AT[Attention Weight]
        V --> CP[Copy Information]
        AT --> CP
        CP --> P[Probability Adjustment]
        B --> P
    end
    P --> C[Next Token 'C']
```

O **mecanismo de c√≥pia** opera atrav√©s de tr√™s componentes principais [^8]:

1. **Componente de Query-Key:**
   $$\text{score}(B,A) = \frac{q_B^T k_A}{\sqrt{d_k}}$$
   onde $q_B$ √© o vetor de query do token B e $k_A$ √© o vetor de key do token A.

2. **Componente de Value:**
   $$v_{copied} = \alpha_{BA} W_V x_A$$
   onde $\alpha_{BA}$ √© o peso de aten√ß√£o normalizado.

3. **Componente de Output:**
   $$\Delta p(C) = W_U W_O v_{copied}$$
   que representa o ajuste na probabilidade do pr√≥ximo token C.

> ‚úîÔ∏è **Insight Importante**: A efici√™ncia deste mecanismo depende da capacidade do modelo em aprender matrizes $W_O W_V$ que preservem informa√ß√µes relevantes durante a c√≥pia.
>
> üìê **Coment√°rio Avan√ßado:** O **mecanismo de c√≥pia** ilustra como os transformers podem replicar informa√ß√µes importantes de tokens anteriores para influenciar a predi√ß√£o atual. **Geometricamente, isso equivale a projetar o vetor de embedding de um token anterior ao longo de dire√ß√µes determinadas pelos vetores de aten√ß√£o, e ent√£o combinar essa proje√ß√£o com o vetor do token atual para ajustar a probabilidade de sa√≠da.** Essa opera√ß√£o envolve uma s√©rie de transforma√ß√µes lineares que permitem a transfer√™ncia e modula√ß√£o de informa√ß√µes no espa√ßo vetorial, demonstrando a efici√™ncia das opera√ß√µes matriciais em capturar depend√™ncias sequenciais.

### 6. An√°lise Te√≥rica da Composi√ß√£o de Bigramas e Skip-Trigramas

**Teorema 1:** *Em um transformer de uma camada, a probabilidade final de um token C pode ser decomposta em:*

$$p(C|contexto) = p_{bigram}(C|B) + \sum_{A \in contexto} p_{skip}(C|A,B)$$

onde:
- $p_{bigram}(C|B)$ vem do termo direto $W_U W_E$
- $p_{skip}(C|A,B)$ vem dos termos de aten√ß√£o

**Prova:**
1. Do path expansion trick, temos:
   $$T = Id \otimes W_U W_E + \sum_{h \in H} A^h \otimes (W_U W_{OV}^h W_E)$$

2. Para um token espec√≠fico C:
   $$\log p(C) = (W_U W_E)_C + \sum_{h \in H} \sum_{A \in contexto} \alpha_{BA}^h (W_U W_{OV}^h W_E)_C$$

3. O primeiro termo corresponde ao modelo de bigramas, e a soma corresponde aos skip-trigramas. $\blacksquare$

[^7]: "Intuitively, this is because each attention head can selectively attend from the present token ('B') to a previous token ('A') and copy information to adjust the probability of possible next tokens ('C')." *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^8]: "The attention pattern is a function of both the source and destination token, but once a destination token has decided how much to attend to a source token, the effect on the output is solely a function of that source token." *(Trecho de A Mathematical Framework for Transformer Circuits)*

### 7. O Path Expansion Trick: Uma An√°lise Aprofundada

O **Path Expansion Trick** √© uma t√©cnica matem√°tica fundamental que nos permite decompor um transformer de uma camada em seus componentes essenciais [^9]. A arquitetura b√°sica consiste em tr√™s est√°gios principais:

1. **Token Embedding:**
   $$x_0 = W_Et$$

2. **Camada de Aten√ß√£o:**
   $$x_1 = x_0 + \sum_{h \in H} h(x_0)$$

3. **Unembedding para Logits:**
   $$T(t) = W_Ux_1$$

```mermaid
graph TD
    A[Tokens de Entrada] --> B[Embedding x‚ÇÄ = W‚ÇëT]
    B --> C[Residual Stream]
    C --> D1[Cabe√ßa de Aten√ß√£o h‚ÇÅ]
    C --> D2[Cabe√ßa de Aten√ß√£o h‚ÇÇ]
    C --> D3[...]
    D1 --> E[Soma no Residual Stream]
    D2 --> E
    D3 --> E
    E --> F["x‚ÇÅ = x‚ÇÄ + Œ£h‚ààH h(x‚ÇÄ)"]
    F --> G[Unembedding W·µ§X‚ÇÅ]
    G --> H[Logits Finais]
```

**Lemma 3:** *O fluxo de informa√ß√£o em um transformer de uma camada pode ser representado como uma composi√ß√£o de transforma√ß√µes lineares e n√£o-lineares, ==onde a √∫nica n√£o-linearidade est√° na fun√ß√£o softmax dos padr√µes de aten√ß√£o.==*

**Prova:**

1. O embedding √© linear: $x_0 = W_Et$
2. Cada cabe√ßa de aten√ß√£o h pode ser decomposta em:
   - Transforma√ß√£o linear para queries e keys
   - Softmax (n√£o-linear)
   - Transforma√ß√£o linear para values
3. A soma no residual stream √© linear
4. O unembedding √© linear
  Portanto, a √∫nica n√£o-linearidade est√° na fun√ß√£o softmax. $\blacksquare$

> üìê **Coment√°rio Avan√ßado:** O **Path Expansion Trick** destaca que a maior parte das opera√ß√µes em um transformer de uma camada s√£o lineares, com a n√£o-linearidade confinada √† fun√ß√£o softmax na aten√ß√£o. ==**Geometricamente, isso significa que o fluxo de informa√ß√£o pode ser rastreado atrav√©s de transforma√ß√µes lineares no espa√ßo vetorial, permitindo que trajet√≥rias distintas de processamento sejam identificadas e analisadas.**== Essa caracter√≠stica facilita a compreens√£o de como a informa√ß√£o √© combinada e transformada ao longo do modelo, j√° que as opera√ß√µes lineares preservam a estrutura do espa√ßo e permitem a aplica√ß√£o direta de t√©cnicas anal√≠ticas da √Ålgebra Linear.

### 8. Representa√ß√£o Tensorial do Transformer

Usando nota√ß√£o tensorial, podemos representar o transformer como um produto de tr√™s termos fundamentais [^10]:

$$T = Id \otimes W_U \cdot \left(Id + \sum_{h \in H} A^h \otimes W_{OV}^h\right) \cdot Id \otimes W_E$$

Este formato tensorial nos permite:

1. **Separar Opera√ß√µes Espaciais e de Feature:**
   - $Id$ opera no espa√ßo posicional
   - $W_U$, $W_{OV}^h$, $W_E$ operam no espa√ßo de features

2. **Identificar Caminhos de Informa√ß√£o:**
   - Caminho direto: $Id \otimes W_U W_E$
   - Caminhos de aten√ß√£o: $A^h \otimes (W_U W_{OV}^h W_E)$

> ‚ö†Ô∏è **Observa√ß√£o Importante**: A nota√ß√£o tensorial revela que cada cabe√ßa de aten√ß√£o opera independentemente e suas contribui√ß√µes s√£o somadas no residual stream.

**Corol√°rio 2:** *A expans√£o do produto tensorial resulta em termos que correspondem a diferentes caminhos atrav√©s do modelo, cada um contribuindo para diferentes aspectos do processamento de linguagem.*

**Prova Esquem√°tica:**
1. O termo direto $Id \otimes W_U W_E$ captura estat√≠sticas de bigramas
2. Os termos de aten√ß√£o $A^h \otimes (W_U W_{OV}^h W_E)$ capturam skip-trigramas
3. A soma desses termos permite que o modelo combine diferentes tipos de informa√ß√£o contextual. $\blacksquare$

> üìê **Coment√°rio Avan√ßado:** A representa√ß√£o tensorial do transformer permite separar claramente as opera√ß√µes que atuam nos espa√ßos posicionais das que atuam nos espa√ßos de caracter√≠sticas. **Geometricamente, isso equivale a considerar o espa√ßo de estados do modelo como um produto tensorial de espa√ßos vetoriais, onde transforma√ß√µes lineares podem ser aplicadas independentemente em cada componente.** Essa separa√ß√£o facilita a an√°lise das intera√ß√µes entre posi√ß√µes na sequ√™ncia e das transforma√ß√µes aplicadas aos embeddings dos tokens, proporcionando uma vis√£o mais estruturada e modular do funcionamento interno do modelo.

[^9]: "Recall that a one-layer attention-only transformer consists of a token embedding, followed by an attention layer (which independently applies attention heads), and finally an unembedding." *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^10]: "Using tensor notation and the alternative representation of attention heads we previously derived, we can represent the transformer as a product of three terms." *(Trecho de A Mathematical Framework for Transformer Circuits)*

### 9. Expans√£o do Produto e An√°lise dos Termos

A expans√£o do produto do transformer nos fornece uma decomposi√ß√£o fundamental que transforma um produto de camadas em uma soma de caminhos end-to-end [^11]. Esta expans√£o pode ser escrita como:

$$T = \text{Id} \otimes W_U W_E + \sum_{h \in H} A^h \otimes (W_U W_{OV}^h W_E)$$

Cada componente tem uma interpreta√ß√£o espec√≠fica:

1. **Token Unembedding ($\text{Id} \otimes W_U$):**
   - Mapeia vetores do residual stream para logits
   - Mant√©m a estrutura posicional atrav√©s do $\text{Id}$

2. **Camada de Aten√ß√£o ($\text{Id} + \sum_{h \in H} A^h \otimes W_{OV}^h$):**
   - Cada cabe√ßa opera independentemente
   - Resultados s√£o somados no residual stream
   - $A^h$ controla o padr√£o de aten√ß√£o

3. **Token Embedding ($\text{Id} \otimes W_E$):**
   - Mapeia tokens para vetores do residual stream
   - Preserva estrutura posicional

```mermaid
graph TD
    subgraph "Expans√£o do Produto"
        A[Termo Direto] --> B[Id ‚äó W·µ§W‚Çë]
        C[Termos de Aten√ß√£o] --> D["Œ£ A ∞ ‚äó (W·µ§W_OV^h W‚Çë)"]
        B --> E[Bigramas]
        D --> F[Skip-trigramas]
    end
    subgraph "Padr√µes de Aten√ß√£o"
        G[Token Input] --> H["t^T"]
        I[Matriz QK] --> J["W‚Çë^T W_QK^h W‚Çë"]
        H --> K[Softmax*]
        J --> K
        K --> L[A ∞]
    end
```

**Lemma 4:** *O padr√£o de aten√ß√£o $A^h$ para cada cabe√ßa √© determinado por uma forma bilinear nos tokens de entrada.*

**Prova:**
$$A^h = \text{softmax}^*(t^T \cdot W_E^T W_{QK}^h W_E \cdot t)$$

1. $W_E^T W_{QK}^h W_E$ define uma forma bilinear
2. O produto com $t$ em ambos os lados produz scores de aten√ß√£o
3. softmax* normaliza os scores com masking autoregressive
$\blacksquare$

### 10. An√°lise dos Padr√µes de Aten√ß√£o

Os **padr√µes de aten√ß√£o** s√£o produzidos multiplicando pares de tokens atrav√©s de diferentes lados de $W_{QK}^h$ [^12]:

$$A^h = \text{softmax}^*(t^T \cdot W_E^T W_{QK}^h W_E \cdot t)$$

> üí° **Insight Cr√≠tico**: O uso de softmax com masking autoregressive ($\text{softmax}^*$) garante que cada token s√≥ pode atender a tokens anteriores ou a si mesmo.

**Corol√°rio 3:** *A capacidade do modelo de capturar depend√™ncias de longo alcance √© limitada pela composi√ß√£o das matrizes de embedding e QK.*

Seja $v_i$ o embedding do token i e $W_{QK}^h$ a matriz QK da cabe√ßa h. A for√ßa da depend√™ncia entre os tokens i e j √© proporcional a:

$$\text{score}(i,j) = v_i^T W_{QK}^h v_j$$

Esta formula√ß√£o nos permite analisar:
1. Como diferentes tokens podem interagir
2. Quais padr√µes lingu√≠sticos podem ser capturados
3. Limita√ß√µes estruturais do modelo

> üìê **Coment√°rio Avan√ßado:** Os padr√µes de aten√ß√£o resultantes da multiplica√ß√£o de pares de tokens atrav√©s de $WQKhW_{QK}^hWQKh$ enfatizam o papel das formas bilineares em **√Ålgebra Linear**. **Geometricamente, o c√°lculo das pontua√ß√µes de aten√ß√£o pode ser visto como a avalia√ß√£o da similaridade entre vetores de embeddings projetados, onde $WQKhW_{QK}^hWQKh$ atua como uma transforma√ß√£o que real√ßa certas dire√ß√µes no espa√ßo vetorial.** Isso permite que o modelo capture rela√ß√µes espec√≠ficas entre tokens, baseando-se em caracter√≠sticas aprendidas durante o treinamento, e demonstra como opera√ß√µes bilineares s√£o fundamentais para a funcionalidade dos mecanismos de aten√ß√£o.

[^11]: "Our key trick is to simply expand the product. This transforms the product (where every term corresponds to a layer), into a sum where every term corresponds to an end-to-end path." *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^12]: "Attention pattern logits are produced by multiplying pairs of tokens through different sides of W^h_QK." *(Trecho de A Mathematical Framework for Transformer Circuits)*

### 11. An√°lise dos Termos End-to-End e Suas Contribui√ß√µes

O comportamento global do modelo pode ser compreendido atrav√©s da an√°lise independente de cada termo do path expansion, que se combinam aditivamente para criar o comportamento final do modelo [^13]. Esta decomposi√ß√£o nos permite uma an√°lise trat√°vel e sistem√°tica.

#### 11.1 O Termo do Caminho Direto

O termo do **caminho direto** ($\text{Id} \otimes W_U W_E$) √© especialmente interessante porque:

1. √â id√™ntico ao termo encontrado em transformers de zero camadas
2. N√£o move informa√ß√£o entre posi√ß√µes (devido ao $\text{Id}$)
3. Contribui exclusivamente para estat√≠sticas de bigramas [^14]

**Teorema 2:** *O termo do caminho direto $\text{Id} \otimes W_U W_E$ aproxima a log-probabilidade de bigramas do corpus de treinamento.*

**Prova:**
1. Seja $p(w_j|w_i)$ a probabilidade real de bigrama no corpus
2. O termo direto produz logits: $l_{ij} = (W_U W_E)_{ij}$
3. Minimizar a cross-entropy resulta em:
   $$l_{ij} \approx \log p(w_j|w_i) + c$$
   onde c √© uma constante de normaliza√ß√£o
   $\blacksquare$

```mermaid
graph TD
    subgraph "Decomposi√ß√£o de Termos"
        A[Entrada] --> B[Caminho Direto]
        A --> C[Caminhos de Aten√ß√£o]
        B --> D[Estat√≠sticas de Bigramas]
        C --> E[Skip-trigramas]
        D --> F[Comportamento Final]
        E --> F
    end
    
    subgraph "Caminho Direto"
        G[Token] --> H[Embedding W‚Çë]
        H --> I[Unembedding W·µ§]
        I --> J[Logits]
    end
```

#### 11.2 Princ√≠pio da Aditividade

**Lemma 5:** *Os diferentes termos do path expansion podem ser analisados independentemente e suas contribui√ß√µes se somam linearmente para produzir o comportamento final do modelo.*

**Prova:**
1. Seja $T$ o transformer completo:
   $$T = T_{direct} + \sum_{h \in H} T_h$$
   onde $T_{direct}$ √© o termo direto e $T_h$ s√£o os termos de aten√ß√£o

2. Para qualquer sequ√™ncia de entrada x:
   $$T(x) = T_{direct}(x) + \sum_{h \in H} T_h(x)$$

3. Os logits finais s√£o a soma das contribui√ß√µes individuais:
   $$l_{final} = l_{direct} + \sum_{h \in H} l_h$$
   $\blacksquare$

> ‚ö†Ô∏è **Ponto Crucial**: A linearidade da soma permite que cada termo seja otimizado de forma relativamente independente durante o treinamento.

#### 11.3 Preenchimento de Lacunas

Um aspecto importante do termo direto √© sua capacidade de preencher lacunas n√£o capturadas pelos outros termos [^15]. Isto ocorre porque:

1. O termo direto pode aprender estat√≠sticas de bigramas que s√£o:
   - Muito espec√≠ficas para serem capturadas por cabe√ßas de aten√ß√£o
   - N√£o seguem padr√µes regulares de skip-trigramas

2. Durante o treinamento, o termo direto se adapta para complementar as cabe√ßas de aten√ß√£o:
   $$W_U W_E \approx \log p(w_j|w_i) - \sum_{h \in H} \mathbb{E}[l_h(w_i, w_j)]$$
   onde $\mathbb{E}[l_h(w_i, w_j)]$ √© a contribui√ß√£o m√©dia da cabe√ßa h para o bigrama $(w_i, w_j)$

[^13]: "We claim each of these end-to-end path terms is tractable to understand, can be reasoned about independently, and additively combine to create model behavior." *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^14]: "The direct path term also occurred when we looked at the zero-layer transformer. Because it doesn't move information between positions (that's what Id ‚äó ... denotes!), the only thing it can contribute to is the bigram statistics" *(Trecho de A Mathematical Framework for Transformer Circuits)*

[^15]: "and it will fill in missing gaps that other terms don't handle there." *(Trecho de A Mathematical Framework for Transformer Circuits)*
