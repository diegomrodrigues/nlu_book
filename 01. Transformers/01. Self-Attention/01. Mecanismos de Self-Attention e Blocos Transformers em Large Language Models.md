# 01. Mecanismos de Self-Attention e Blocos Transformers em Modelos de Linguagem de Grande Porte

![image-20241114180259385](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20241114180259385.png)

https://claude.site/artifacts/11e8fa92-21ea-4ad3-8979-25105c6f49eb

### Introdu√ß√£o

Os **Transformers** revolucionaram a arquitetura de Modelos de Linguagem de Grande Porte (LLMs), introduzindo uma abordagem inovadora para o processamento de sequ√™ncias de dados [^1]. A caracter√≠stica mais marcante desta arquitetura √© a capacidade de processar sequ√™ncias de entrada atrav√©s de blocos Transformers empilhados. Cada bloco √© uma rede neural multicamada que mapeia sequ√™ncias de vetores de entrada $(x_1, ..., x_n)$ para sequ√™ncias de vetores de sa√≠da $(z_1, ..., z_n)$ de mesmo comprimento [^1].

No cora√ß√£o desta arquitetura est√° o mecanismo de **self-attention**, uma inova√ß√£o que permite √† rede extrair e utilizar informa√ß√µes de contextos arbitrariamente grandes [^1]. Essa capacidade √© crucial no processamento de linguagem natural, onde rela√ß√µes de longa dist√¢ncia entre palavras s√£o fundamentais para a compreens√£o do significado.

### Self-Attention: Fundamentos Te√≥ricos

![image-20241114181658881](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20241114181658881.png)

https://claude.site/artifacts/63210716-f6cc-4829-97b3-d4d013ee577f

A ess√™ncia do mecanismo de self-attention pode ser compreendida atrav√©s de sua formula√ß√£o matem√°tica fundamental. Considere uma sequ√™ncia de vetores de entrada $\{x_n\}$ em um espa√ßo de embedding. O objetivo √© mapear esta sequ√™ncia para outra sequ√™ncia $\{y_n\}$ que capture rela√ß√µes contextuais mais ricas [^2].

**Lema 1 (Representa√ß√£o por Combina√ß√£o Linear):** Para cada vetor de sa√≠da $y_n$, existe uma representa√ß√£o como combina√ß√£o linear dos vetores de entrada, ponderada por coeficientes de aten√ß√£o $a_{nm}$:

$$
y_n = \sum_{m=1}^N a_{nm} x_m
$$

onde os coeficientes $a_{nm}$ satisfazem:

1. $a_{nm} \geq 0$ (n√£o-negatividade)
2. $\sum_{m=1}^N a_{nm} = 1$ (normaliza√ß√£o)

**Prova:**

1. **C√°lculo das Proje√ß√µes:**

   Cada vetor de entrada $x_m$ √© transformado em tr√™s vetores distintos atrav√©s de proje√ß√µes lineares:

   $$
   \begin{aligned}
   q_m &= x_m W^{(q)} \quad \text{(query)} \\
   k_m &= x_m W^{(k)} \quad \text{(key)} \\
   v_m &= x_m W^{(v)} \quad \text{(value)}
   \end{aligned}
   $$

   onde $W^{(q)}, W^{(k)}, W^{(v)}$ s√£o matrizes de peso aprendidas.

2. **C√°lculo dos Scores de Aten√ß√£o:**

   O score de aten√ß√£o entre o elemento na posi√ß√£o $n$ e o elemento na posi√ß√£o $m$ √© dado por:

   $$
   s_{nm} = \frac{q_n \cdot k_m}{\sqrt{d_k}}
   $$

   onde $d_k$ √© a dimens√£o dos vetores de key e query, e $\cdot$ representa o produto escalar.

3. **Aplica√ß√£o do Softmax:**

   Os coeficientes de aten√ß√£o $a_{nm}$ s√£o obtidos aplicando a fun√ß√£o softmax aos scores:

   $$
   a_{nm} = \frac{\exp(s_{nm})}{\sum_{l=1}^N \exp(s_{nl})}
   $$

   Isso garante que:

   - $a_{nm} \geq 0$ (pois a exponencial de qualquer n√∫mero real √© positiva)
   - $\sum_{m=1}^N a_{nm} = 1$ (pois o softmax produz uma distribui√ß√£o de probabilidade)

4. **C√°lculo da Sa√≠da:**

   O vetor de sa√≠da $y_n$ √© ent√£o calculado como a combina√ß√£o ponderada dos valores $v_m$:

   $$
   y_n = \sum_{m=1}^N a_{nm} v_m
   $$

   Como $v_m = x_m W^{(v)}$, podemos expressar $y_n$ em termos dos vetores de entrada $x_m$:

   $$
   y_n = \sum_{m=1}^N a_{nm} x_m W^{(v)}
   $$

   Portanto, $y_n$ √© uma combina√ß√£o linear dos vetores de entrada $x_m$, ponderada pelos coeficientes de aten√ß√£o $a_{nm}$. $\blacksquare$

>  üåÄ **Coment√°rio Avan√ßado:** O **Lema 1** destaca que cada vetor de sa√≠da $y_n$ √© uma combina√ß√£o linear ponderada dos vetores de entrada $x_m$. ==**Essa opera√ß√£o pode ser interpretada geometricamente como uma m√©dia ponderada no espa√ßo vetorial, onde os coeficientes de aten√ß√£o $a_{nm}$ determinam a influ√™ncia de cada vetor na dire√ß√£o resultante.**== Essa m√©dia ponderada permite que o modelo sintetize informa√ß√µes de m√∫ltiplas fontes, posicionando $y_n$ em um ponto do espa√ßo que reflete as rela√ß√µes contextuais relevantes.

A implementa√ß√£o pr√°tica do self-attention envolve tr√™s transforma√ß√µes lineares principais atrav√©s das matrizes $W^{(q)}$, $W^{(k)}$ e $W^{(v)}$, que produzem, respectivamente, os vetores de query, key e value.

```mermaid
graph TD
    X[Entrada X] --> Q[Query = XW<sup>Q</sup>]
    X --> K[Key = XW<sup>K</sup>]
    X --> V[Value = XW<sup>V</sup>]
    Q --> ATT[Scores de Aten√ß√£o]
    K --> ATT
    ATT --> SOFT[Softmax]
    SOFT --> OUT[Sa√≠da]
    V --> OUT
```

A fun√ß√£o de aten√ß√£o escalonada √© ent√£o definida como:

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V
$$

### Arquitetura de Blocos Transformers

Um bloco Transformer completo combina o mecanismo de self-attention com redes feedforward e normaliza√ß√µes em uma estrutura sofisticada [^4]. Cada bloco pode ser representado pela seguinte sequ√™ncia de opera√ß√µes:

**Teorema 1 (Composi√ß√£o de Blocos Transformer):** A transforma√ß√£o realizada por um bloco Transformer pode ser expressa como:

$$
\begin{aligned}
T^1 &= \text{SelfAttention}(X) \\
T^2 &= X + T^1 \quad \text{(Conex√£o Residual)} \\
T^3 &= \text{LayerNorm}(T^2) \\
T^4 &= \text{FFN}(T^3) \quad \text{(Feedforward Network)} \\
T^5 &= T^3 + T^4 \quad \text{(Conex√£o Residual)} \\
H &= \text{LayerNorm}(T^5)
\end{aligned}
$$

**Demonstra√ß√£o:**

1. **Self-Attention:**

   Aplicamos o mecanismo de self-attention aos vetores de entrada $X$, obtendo $T^1$:

   $$
   T^1 = \text{SelfAttention}(X)
   $$

2. **Conex√£o Residual 1:**

   Somamos a sa√≠da do self-attention $T^1$ √† entrada original $X$ para formar $T^2$:

   $$
   T^2 = X + T^1
   $$

   Isso ajuda na propaga√ß√£o do gradiente e evita problemas de vanishing gradients.

3. **Normaliza√ß√£o de Camada 1:**

   Aplicamos a normaliza√ß√£o de camada (LayerNorm) a $T^2$ para obter $T^3$:

   $$
   T^3 = \text{LayerNorm}(T^2)
   $$

   A normaliza√ß√£o melhora a estabilidade e acelera o treinamento.

4. **Rede Feedforward:**

   Passamos $T^3$ por uma rede feedforward (geralmente composta por duas camadas lineares com uma ativa√ß√£o n√£o linear no meio) para obter $T^4$:

   $$
   T^4 = \text{FFN}(T^3)
   $$

5. **Conex√£o Residual 2:**

   Somamos a sa√≠da da feedforward $T^4$ ao tensor $T^3$ para formar $T^5$:

   $$
   T^5 = T^3 + T^4
   $$

6. **Normaliza√ß√£o de Camada 2:**

   Aplicamos a normaliza√ß√£o de camada a $T^5$ para obter a sa√≠da final $H$ do bloco Transformer:

   $$
   H = \text{LayerNorm}(T^5)
   $$

$\blacksquare$

https://claude.site/artifacts/653743b3-f0a3-453c-bb03-db931fc70390

> üåÄ **Coment√°rio Avan√ßado:** A arquitetura dos **blocos Transformers** combina opera√ß√µes de self-attention com redes feedforward e mecanismos de normaliza√ß√£o, criando uma sequ√™ncia de transforma√ß√µes geom√©tricas sofisticadas. **Cada bloco pode ser visto como uma composi√ß√£o de mapeamentos n√£o lineares que realinham os vetores em espa√ßos latentes, enfatizando dire√ß√µes relevantes e atenuando ru√≠dos.** ==As **conex√µes residuais** preservam a integridade das representa√ß√µes originais, enquanto as normaliza√ß√µes asseguram que as distribui√ß√µes dos vetores permane√ßam est√°veis durante o fluxo atrav√©s das camadas.==

[^1]: *"Transformers s√£o formados por pilhas de blocos transformers, cada um dos quais √© uma rede multicamada que mapeia sequ√™ncias de vetores de entrada (x1, ..., xn) para sequ√™ncias de vetores de sa√≠da (z1, ..., zn) do mesmo comprimento."* (Speech and Language Processing, Cap√≠tulo 10)

[^2]: *"A ess√™ncia do mecanismo de aten√ß√£o √© a ideia de comparar um item de interesse a uma cole√ß√£o de outros itens de uma forma que revela sua relev√¢ncia no contexto atual."* (Speech and Language Processing, Cap√≠tulo 10)

[^3]: *"Os coeficientes devem ser n√£o-negativos para evitar situa√ß√µes em que um coeficiente possa se tornar grande e positivo enquanto outro coeficiente compensa tornando-se grande e negativo."* (Speech and Language Processing, Cap√≠tulo 10)

[^4]: *"O c√°lculo do self-attention est√° no n√∫cleo do que √© chamado de bloco transformer, que, al√©m da camada de self-attention, inclui tr√™s outros tipos de camadas."* (Speech and Language Processing, Cap√≠tulo 10)

### Aten√ß√£o como Tabela de Consulta Suave (Soft Lookup)

![image-20241114183100989](C:\Users\diego.rodrigues\AppData\Roaming\Typora\typora-user-images\image-20241114183100989.png)

A interpreta√ß√£o do mecanismo de aten√ß√£o como uma tabela de consulta suave representa uma perspectiva te√≥rica fundamental para compreender seu funcionamento em Modelos de Linguagem de Grande Porte [^5]. Este conceito estabelece uma analogia elegante com estruturas de dados tradicionais, proporcionando uma intui√ß√£o mais profunda sobre o funcionamento interno dos Transformers.

==**Lema 2 (Propriedade de Lookup Suave):** O mecanismo de aten√ß√£o pode ser formalizado como uma generaliza√ß√£o cont√≠nua de uma tabela de consulta discreta, onde:==

1. Cada query $q$ interage com todas as keys $k_i$ simultaneamente.
2. Os pesos de aten√ß√£o $\alpha_i$ formam uma distribui√ß√£o de probabilidade sobre os values.
3. A sa√≠da √© uma m√©dia ponderada dos values.

**Prova:**

1. **Tabela de Consulta Tradicional:**

   Em uma tabela de consulta discreta, para uma query $q$, buscamos a key $k_j$ que corresponde exatamente ou √© mais semelhante a $q$, e retornamos o value associado $v_j$:

   $$
   \text{lookup}(q) = v_j \quad \text{onde} \quad j = \arg\max_i (q \cdot k_i)
   $$

   Aqui, o operador $\arg\max$ seleciona o √≠ndice $j$ que maximiza a similaridade (por exemplo, produto escalar) entre $q$ e $k_i$.

2. **Suaviza√ß√£o com Softmax:**

   ==O mecanismo de aten√ß√£o substitui a opera√ß√£o discreta $\arg\max$ por uma combina√ß√£o ponderada usando o softmax. A sa√≠da √© uma m√©dia ponderada de todos os values, onde os pesos s√£o determinados pela similaridade entre $q$ e cada $k_i$:==
   $$
   \text{attention}(q) = \sum_{i=1}^N \alpha_i v_i
   $$
   
   com
   
   $$
   \alpha_i = \frac{\exp(q \cdot k_i / \sqrt{d_k})}{\sum_{j=1}^N \exp(q \cdot k_j / \sqrt{d_k})}
   $$
   
3. **Compara√ß√£o:**

   - No lookup tradicional, apenas um √∫nico value $v_j$ √© retornado.
   - ==No mecanismo de aten√ß√£o, todos os values $v_i$ contribuem para a sa√≠da, ponderados por $\alpha_i$.==

4. **Conclus√£o:**

   A transi√ß√£o de uma sele√ß√£o discreta para uma m√©dia ponderada cont√≠nua (suave) via softmax caracteriza o mecanismo de aten√ß√£o como um lookup suave. $\blacksquare$

https://claude.site/artifacts/3ef7d043-9f4c-4d0f-9f52-0d693c175a52

> üåÄ **Coment√°rio Avan√ßado:** A interpreta√ß√£o da aten√ß√£o como uma **tabela de consulta suave** oferece uma perspectiva geom√©trica elegante sobre o mecanismo. ==**Em vez de selecionar um √∫nico vetor de refer√™ncia, o modelo efetua uma interpola√ß√£o cont√≠nua no espa√ßo de embeddings, onde a contribui√ß√£o de cada value √© ponderada pela similaridade espacial entre queries e keys.**== Isso permite capturar influ√™ncias m√∫ltiplas e graduais de contextos diversos, resultando em representa√ß√µes mais ricas e robustas.

**Corol√°rio 1:** A sa√≠da do mecanismo de aten√ß√£o para cada posi√ß√£o √© uma combina√ß√£o convexa dos values, garantindo que a magnitude da sa√≠da permane√ßa controlada:
$$
\|O_i\| \leq \sum_{j=1}^N \alpha_{ij} \|v_j\| \leq \max_j \|v_j\|
$$

**Prova:**

1. Como $\alpha_{ij} \geq 0$ e $\sum_{j=1}^N \alpha_{ij} = 1$, temos que $\alpha_{ij}$ forma uma distribui√ß√£o de probabilidade.

2. A norma da sa√≠da √©:

   $$
   \|O_i\| = \left\| \sum_{j=1}^N \alpha_{ij} v_j \right\| \leq \sum_{j=1}^N \alpha_{ij} \|v_j\|
   $$

   (usando a desigualdade triangular)

3. Como $\alpha_{ij} \leq 1$, a soma ponderada das normas √© limitada pelo m√°ximo das normas dos $v_j$.

4. Portanto:

   $$
   \|O_i\| \leq \max_j \|v_j\|
   $$

$\blacksquare$

[^5]: *"Um mecanismo de lookup suave onde a rede pode olhar amplamente no contexto e determinar como integrar a representa√ß√£o de palavras nesse contexto."* (Speech and Language Processing, Cap√≠tulo 10)

[^6]: *"A normaliza√ß√£o por softmax significa que a distribui√ß√£o n√£o √© sim√©trica, embora a matriz de aten√ß√£o resultante n√£o seja ela pr√≥pria sim√©trica."* (Deep Learning Foundations and Concepts, Cap√≠tulo 12)

### An√°lise Te√≥rica da Complexidade do Lookup Suave

**Pergunta Te√≥rica:** Como a complexidade computacional do mecanismo de aten√ß√£o se compara com uma tabela de consulta tradicional?

**Lema 3 (Complexidade de Lookup):** Para uma sequ√™ncia de comprimento $N$ e dimens√£o de embedding $d$, o mecanismo de aten√ß√£o possui:

1. **Complexidade de Mem√≥ria:** $O(N^2)$ para armazenar os pesos de aten√ß√£o.
2. **Complexidade Computacional:** $O(N^2 d)$ para o c√°lculo completo.

**Prova:**

1. **C√°lculo dos Scores de Aten√ß√£o:**

   O c√°lculo de $Q K^\top$ envolve multiplicar uma matriz $Q \in \mathbb{R}^{N \times d_k}$ por uma matriz $K^\top \in \mathbb{R}^{d_k \times N}$, resultando em uma matriz de scores $S \in \mathbb{R}^{N \times N}$. Isso requer $O(N^2 d_k)$ opera√ß√µes.

2. **Aplica√ß√£o do Softmax:**

   O softmax √© aplicado a cada linha da matriz de scores $S$, o que envolve $O(N^2)$ opera√ß√µes.

3. **Multiplica√ß√£o pelos Values:**

   Multiplicar a matriz de aten√ß√£o normalizada $A \in \mathbb{R}^{N \times N}$ pela matriz de values $V \in \mathbb{R}^{N \times d_v}$ resulta na sa√≠da $O \in \mathbb{R}^{N \times d_v}$. Isso requer $O(N^2 d_v)$ opera√ß√µes.

4. **Complexidade Total:**

   Somando as opera√ß√µes acima e considerando que $d_k$ e $d_v$ s√£o geralmente proporcionais a $d$, a complexidade computacional total √© $O(N^2 d)$.

5. **Complexidade de Mem√≥ria:**

   Armazenar a matriz de aten√ß√£o $A \in \mathbb{R}^{N \times N}$ requer $O(N^2)$ espa√ßo de mem√≥ria.

$\blacksquare$

<Desenho: Um gr√°fico mostrando o crescimento da **complexidade computacional** e da **complexidade de mem√≥ria** em fun√ß√£o do **comprimento da sequ√™ncia** $N$. Use curvas para representar $O(N)$, $O(N \log N)$ e **$O(N^2)$**, destacando que o mecanismo de aten√ß√£o apresenta crescimento quadr√°tico. *Inclua eixos claramente rotulados, legendas para cada curva e destaque a implica√ß√£o de crescimento quadr√°tico para sequ√™ncias longas.*>

> ‚ùó **Limita√ß√£o Cr√≠tica**: A complexidade quadr√°tica em rela√ß√£o ao comprimento da sequ√™ncia representa um gargalo significativo para o processamento de sequ√™ncias muito longas [^7].

[^7]: *"A aten√ß√£o √© quadr√°tica no comprimento da entrada, j√° que em cada camada precisamos computar produtos escalares entre cada par de tokens na entrada."* (Speech and Language Processing, Cap√≠tulo 10)

### Os Tr√™s Pap√©is dos Embeddings na Aten√ß√£o

O mecanismo de aten√ß√£o nos Transformers apresenta uma caracter√≠stica √∫nica e sofisticada: cada embedding de entrada desempenha tr√™s pap√©is distintos e complementares durante o processamento [^8]. Essa tr√≠ade de fun√ß√µes ‚Äî **query**, **key** e **value** ‚Äî forma a base do mecanismo de self-attention e merece uma an√°lise te√≥rica aprofundada.

```mermaid
graph TD
    E[Embedding de Entrada] --> Q[Query: Foco da Aten√ß√£o]
    E --> K[Key: Elemento para Compara√ß√£o]
    E --> V[Value: Informa√ß√£o para Computa√ß√£o]
    Q --> S[Scores de Similaridade]
    K --> S
    S --> A["Pesos de Aten√ß√£o (Softmax)"]
    A --> O[Sa√≠da]
    V --> O
```

**Teorema 3 (Decomposi√ß√£o Tripartite dos Embeddings):** Para cada embedding de entrada $x$, existem tr√™s transforma√ß√µes lineares distintas que produzem suas representa√ß√µes como query, key e value:

$$
\begin{aligned}
q &= x W^Q \\
k &= x W^K \\
v &= x W^V
\end{aligned}
$$

onde $W^Q, W^K \in \mathbb{R}^{d \times d_k}$ e $W^V \in \mathbb{R}^{d \times d_v}$ s√£o matrizes de par√¢metros aprend√≠veis.

**Lema 4 (Propriedade de Assimetria):** A separa√ß√£o em queries e keys permite que o modelo capture rela√ß√µes assim√©tricas na linguagem.

**Prova:**

1. **Defini√ß√£o dos Vetores:**

   Para dois tokens $x_i$ e $x_j$, calculamos suas proje√ß√µes:

   $$
   \begin{aligned}
   q_i &= x_i W^Q \\
   k_j &= x_j W^K
   \end{aligned}
   $$

2. **C√°lculo dos Scores de Aten√ß√£o:**

   A aten√ß√£o que o token $x_i$ presta ao token $x_j$ √© dada por:

   $$
   \alpha_{ij} = \frac{\exp(q_i^\top k_j / \sqrt{d_k})}{\sum_{m=1}^N \exp(q_i^\top k_m / \sqrt{d_k})}
   $$

   De forma similar, a aten√ß√£o que $x_j$ presta a $x_i$ √©:

   $$
   \alpha_{ji} = \frac{\exp(q_j^\top k_i / \sqrt{d_k})}{\sum_{m=1}^N \exp(q_j^\top k_m / \sqrt{d_k})}
   $$

3. **Assimetria:**

   Como $q_i$ e $k_i$ s√£o obtidos atrav√©s de proje√ß√µes diferentes ($W^Q \neq W^K$), em geral $q_i^\top k_j \neq q_j^\top k_i$. Portanto, $\alpha_{ij} \neq \alpha_{ji}$, estabelecendo a assimetria.

4. **Conclus√£o:**

   Essa assimetria √© crucial para modelar rela√ß√µes lingu√≠sticas onde a rela√ß√£o entre dois tokens n√£o √© necessariamente sim√©trica, como em depend√™ncias gramaticais ou rela√ß√µes sem√¢nticas direcionais.

$\blacksquare$

<Desenho: Um diagrama mostrando um **vetor de entrada** $x_i$ sendo projetado em tr√™s vetores distintos: **query** $q_i$, **key** $k_i$ e **value** $v_i$, atrav√©s das matrizes de proje√ß√£o **$W^Q$**, **$W^K$** e **$W^V$**. Ilustre como o **query** $q_i$ interage com as **keys** $k_j$ para calcular os **scores de aten√ß√£o**, resultando nos **pesos de aten√ß√£o** $\alpha_{ij}$, que s√£o usados para ponderar os **values** $v_j$ e produzir a sa√≠da. *Utilize setas para representar as proje√ß√µes e intera√ß√µes, e inclua r√≥tulos para cada componente e opera√ß√£o.*>

[^8]: *"Durante o curso do processo de aten√ß√£o, cada embedding de entrada desempenha tr√™s pap√©is diferentes: como foco atual da aten√ß√£o (query), como entrada precedente sendo comparada (key), e como valor usado para computar a sa√≠da (value)."* (Speech and Language Processing, Cap√≠tulo 10)

[^9]: *"Para capturar esses tr√™s pap√©is diferentes, os transformers introduzem matrizes de peso W^Q, W^K, e W^V."* (Speech and Language Processing, Cap√≠tulo 10)

### Transforma√ß√µes Lineares e Dimensionalidade em Aten√ß√£o

O mecanismo de aten√ß√£o utiliza transforma√ß√µes lineares espec√≠ficas para projetar cada vetor de entrada em suas representa√ß√µes correspondentes de query, key e value [^12]. Esta se√ß√£o explora em profundidade as propriedades matem√°ticas e dimensionais dessas transforma√ß√µes.

```mermaid
graph TD
    subgraph "Espa√ßo de Entrada (d)"
        X[Vetores de Entrada xi]
    end
    subgraph "Matrizes de Proje√ß√£o"
        WQ[W<sup>Q</sup> ‚àà ‚Ñù<sup>d√ód<sub>k</sub></sup>]
        WK[W<sup>K</sup> ‚àà ‚Ñù<sup>d√ód<sub>k</sub></sup>]
        WV[W<sup>V</sup> ‚àà ‚Ñù<sup>d√ód<sub>v</sub></sup>]
    end
    subgraph "Espa√ßos Projetados"
        Q["Espa√ßo de Query (d<sub>k</sub>)"]
        K["Espa√ßo de Key (d<sub>k</sub>)"]
        V["Espa√ßo de Value (d<sub>v</sub>)"]
    end
    X --> |"xi W<sup>Q</sup>"| Q
    X --> |"xi W<sup>K</sup>"| K
    X --> |"xi W<sup>V</sup>"| V
```

**Teorema 5 (Propriedades Dimensionais das Transforma√ß√µes):** Para um modelo Transformer com dimens√£o de embedding $d$, as transforma√ß√µes lineares satisfazem:

$$
\begin{aligned}
q_i &= x_i W^Q & W^Q &\in \mathbb{R}^{d \times d_k} \\
k_i &= x_i W^K & W^K &\in \mathbb{R}^{d \times d_k} \\
v_i &= x_i W^V & W^V &\in \mathbb{R}^{d \times d_v}
\end{aligned}
$$

onde $d_k$ e $d_v$ s√£o as dimens√µes dos espa√ßos de query/key e value, respectivamente [^13].

<Desenho: Um diagrama que representa as transforma√ß√µes lineares dos **vetores de entrada** $x_i$ nos espa√ßos projetados. Mostre o **espa√ßo de entrada** de dimens√£o $d$, as matrizes de proje√ß√£o **$W^Q$**, **$W^K$**, **$W^V$**, e os **espa√ßos projetados** de dimens√µes $d_k$ (para **queries** e **keys**) e $d_v$ (para **values**). Ilustre como cada $x_i$ √© transformado em $q_i$, $k_i$ e $v_i$ atrav√©s dessas matrizes. *Use blocos para os espa√ßos e matrizes, setas para as transforma√ß√µes, e inclua r√≥tulos claros para cada dimens√£o e componente.*>

**Teorema 6 (Vari√¢ncia dos Produtos Escalares):** Para vetores query e key aleatoriamente inicializados com m√©dia zero e vari√¢ncia unit√°ria, a vari√¢ncia do produto escalar √© proporcional a $d_k$:

$$
\text{Var}(q_i^\top k_j) = d_k
$$

**Prova:**

1. **Assumindo Vetores Aleat√≥rios:**

   - Seja cada componente de $q_i$ e $k_j$ distribu√≠do como $\mathcal{N}(0, 1)$.

2. **Produto Escalar:**

   - O produto escalar √© $q_i^\top k_j = \sum_{l=1}^{d_k} q_{il} k_{jl}$.

3. **C√°lculo da Vari√¢ncia:**

   - Como os $q_{il}$ e $k_{jl}$ s√£o independentes e identicamente distribu√≠dos, a vari√¢ncia do produto escalar √© a soma das vari√¢ncias dos produtos individuais:

     $$
     \text{Var}(q_i^\top k_j) = \sum_{l=1}^{d_k} \text{Var}(q_{il} k_{jl}) = \sum_{l=1}^{d_k} (\text{Var}(q_{il}) \times \text{Var}(k_{jl})) = d_k \times 1 \times 1 = d_k
     $$

$\blacksquare$

**Corol√°rio 3:** A escala $\frac{1}{\sqrt{d_k}}$ na fun√ß√£o de aten√ß√£o √© necess√°ria para manter a vari√¢ncia dos scores em um intervalo adequado para o softmax:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^\top}{\sqrt{d_k}}\right) V
$$

**Prova:**

1. **Sem Escalonamento:**

   - Sem o fator $\frac{1}{\sqrt{d_k}}$, os scores teriam vari√¢ncia $d_k$.
   - Para valores grandes de $d_k$, os scores podem ter valores muito altos, levando a problemas num√©ricos no softmax (por exemplo, gradientes muito pequenos ou satura√ß√£o).

2. **Com Escalonamento:**

   - Dividindo por $\sqrt{d_k}$, a vari√¢ncia dos scores torna-se 1.
   - Isso mant√©m os valores dos scores em uma faixa que √© apropriada para o softmax, evitando problemas de estabilidade num√©rica.

$\blacksquare$

[^12]: *"Para capturar estes tr√™s pap√©is diferentes, os transformers introduzem matrizes de peso W^Q, W^K e W^V. Estes pesos ser√£o usados para projetar cada vetor de entrada x_i em uma representa√ß√£o de seu papel como key, query ou value."* (Speech and Language Processing, Cap√≠tulo 10)

[^13]: *"As entradas x e sa√≠das y dos transformers, assim como os vetores intermedi√°rios ap√≥s as v√°rias camadas como o vetor de sa√≠da de aten√ß√£o a, todos t√™m a mesma dimensionalidade 1√ód."* (Speech and Language Processing, Cap√≠tulo 10)

[^14]: *"No trabalho original do Transformer (Vaswani et al., 2017), d era 512, d_k e d_v eram ambos 64. As formas das matrizes de transforma√ß√£o s√£o ent√£o W^Q ‚àà ‚Ñù^{d√ód_k}, W^K ‚àà ‚Ñù^{d√ód_k}, e W^V ‚àà ‚Ñù^{d√ód_v}."* (Speech and Language Processing, Cap√≠tulo 10)

### Escalonamento da Aten√ß√£o e Formula√ß√£o Completa

O mecanismo de self-attention incorpora um componente crucial de escalonamento para garantir estabilidade num√©rica e efici√™ncia no treinamento [^15]. Esta se√ß√£o apresenta a formula√ß√£o matem√°tica completa e analisa suas propriedades te√≥ricas.

**Teorema 7 (Formula√ß√£o Completa do Self-Attention):** Para um vetor de entrada $x_i$, o c√°lculo completo do self-attention segue quatro etapas fundamentais:

1. **Proje√ß√£o:**

   $$
   \begin{aligned}
   q_i &= x_i W^Q \\
   k_i &= x_i W^K \\
   v_i &= x_i W^V
   \end{aligned}
   $$

2. **C√°lculo dos Scores:**

   $$
   \text{score}(x_i, x_j) = \frac{q_i^\top k_j}{\sqrt{d_k}}
   $$

3. **Normaliza√ß√£o (Softmax):**

   $$
   \alpha_{ij} = \frac{\exp(\text{score}(x_i, x_j))}{\sum_{m=1}^N \exp(\text{score}(x_i, x_m))}
   $$

4. **Agrega√ß√£o:**

   $$
   a_i = \sum_{j=1}^N \alpha_{ij} v_j
   $$

<Desenho: Um diagrama detalhando o **processo completo do self-attention**. Mostre a entrada $x_i$ sendo projetada em **$q_i$**, **$k_i$** e **$v_i$** atrav√©s das matrizes **$W^Q$**, **$W^K$**, **$W^V$**. Ilustre o c√°lculo dos **scores** $q_i^\top k_j$, a divis√£o pelo **fator de escalonamento** $\sqrt{d_k}$, a aplica√ß√£o do **softmax** para obter os **pesos de aten√ß√£o** $\alpha_{ij}$, e a agrega√ß√£o dos **values** $v_j$ para formar $a_i$. *Use blocos para cada etapa, setas para o fluxo, e destaque o fator de escalonamento na equa√ß√£o dos scores.*>

**Lema 6 (Propriedade do Escalonamento):** O fator de escala $\frac{1}{\sqrt{d_k}}$ mant√©m a vari√¢ncia dos scores em uma faixa adequada independentemente da dimensionalidade.

**Prova:**

1. **Vari√¢ncia Sem Escalonamento:**

   - Sem escalonamento, $\text{Var}(q_i^\top k_j) = d_k$ (como demonstrado no Teorema 6).

2. **Vari√¢ncia Com Escalonamento:**

   - Com escalonamento, temos:

     $$
     \text{Var}\left(\frac{q_i^\top k_j}{\sqrt{d_k}}\right) = \frac{\text{Var}(q_i^\top k_j)}{d_k} = \frac{d_k}{d_k} = 1
     $$

3. **Benef√≠cio:**

   - Manter a vari√¢ncia dos scores em torno de 1 evita que os valores das exponenciais no softmax sejam muito grandes ou muito pequenos, o que poderia causar problemas num√©ricos.

$\blacksquare$

> ‚ö†Ô∏è **Ponto Crucial**: A exponencia√ß√£o de valores muito grandes no softmax pode causar problemas num√©ricos e perda efetiva de gradientes durante o treinamento [^15].

### An√°lise do Gradiente e Estabilidade Num√©rica

**Pergunta Te√≥rica:** Como o escalonamento afeta o comportamento dos gradientes durante o treinamento?

**Teorema 8 (Comportamento do Gradiente):** O gradiente da fun√ß√£o softmax em rela√ß√£o aos scores √© dado por:

$$
\frac{\partial \text{softmax}(z)_i}{\partial z_j} = \text{softmax}(z)_i \left( \delta_{ij} - \text{softmax}(z)_j \right)
$$

onde $\delta_{ij}$ √© o delta de Kronecker (igual a 1 se $i = j$, e 0 caso contr√°rio).

**Corol√°rio 4:** Para scores n√£o escalonados $s$, a magnitude do gradiente √© proporcional a $\exp(\|s\|)$. Com escalonamento, a magnitude √© proporcional a $\exp(\|s\| / \sqrt{d_k})$, evitando a explos√£o dos gradientes para grandes $d_k$.

**Prova:**

1. **Sem Escalonamento:**

   - Se os scores $s$ t√™m vari√¢ncia alta (proporcional a $d_k$), ent√£o $\|s\|$ √© grande.
   - A fun√ß√£o softmax envolve $\exp(s_i)$, ent√£o gradientes em rela√ß√£o a $s_i$ tamb√©m envolvem $\exp(s_i)$.
   - Portanto, a magnitude do gradiente cresce exponencialmente com $\|s\|$.

2. **Com Escalonamento:**

   - Dividindo os scores por $\sqrt{d_k}$, reduzimos $\|s\|$ para $\|s\| / \sqrt{d_k}$.
   - Isso controla a magnitude dos gradientes, evitando explos√£o durante o treinamento.

$\blacksquare$

<Desenho: Um gr√°fico comparando a **magnitude dos gradientes** em fun√ß√£o da dimens√£o $d_k$, com e sem o **fator de escalonamento**. Mostre que, sem escalonamento, os gradientes aumentam exponencialmente, enquanto com escalonamento permanecem est√°veis. *Use curvas distintas para cada caso, inclua eixos rotulados e uma legenda explicativa.*>

### Processamento Paralelo Eficiente

O c√°lculo do self-attention pode ser paralelizado eficientemente usando opera√ß√µes matriciais:

$$
A = \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
$$

onde, para uma sequ√™ncia de comprimento $N$:

- $Q, K \in \mathbb{R}^{N \times d_k}$
- $V \in \mathbb{R}^{N \times d_v}$
- $A \in \mathbb{R}^{N \times d_v}$

<Desenho: Um diagrama ilustrando o **processamento paralelo** do mecanismo de aten√ß√£o. Mostre as matrizes **$Q$**, **$K$** e **$V$** sendo usadas em opera√ß√µes matriciais para calcular **$Q K^\top$**, seguido pela aplica√ß√£o do **softmax** para obter a matriz de **pesos de aten√ß√£o**, e a multiplica√ß√£o por **$V$** para produzir a sa√≠da **$A$**. *Use representa√ß√µes matriciais, setas indicando o fluxo das opera√ß√µes e destaque como as opera√ß√µes podem ser computadas simultaneamente.*>

[^15]: *"O resultado de um produto escalar pode ser um valor arbitrariamente grande (positivo ou negativo). A exponencia√ß√£o de valores grandes pode levar a problemas num√©ricos e a uma perda efetiva de gradientes durante o treinamento. Para evitar isso, escalamos para baixo o resultado do produto escalar, dividindo-o por um fator relacionado ao tamanho dos embeddings."* (Speech and Language Processing, Cap√≠tulo 10)